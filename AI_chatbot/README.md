# ğŸ› ï¸ Using ollama to run LLM model locally. Use python
