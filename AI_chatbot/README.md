# ğŸ› ï¸ Using ollama to run LLM model locally. Use langchain and steamlit in python 
